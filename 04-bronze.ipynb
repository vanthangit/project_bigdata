{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b950598-b774-4cbc-a98b-bb3ddff04fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b89be1-1216-4fed-b8ed-cfbe3e2cbcdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "    def __init__(self, env):\n",
    "        Conf = Config()\n",
    "        self.landing_zone = Conf.base_data_path\n",
    "        self.catalog = env\n",
    "        self.db_name = Conf.db_name\n",
    "        self.initialized = False\n",
    "\n",
    "    def create_db(self):\n",
    "        print(f\"Creating the database {self.catalog}.{self.db_name}...\", end='')\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "        self.initialized = True\n",
    "        print(\"Done\")   \n",
    "\n",
    "# --------------------------------INSERT DATA--------------------------------\n",
    "    def insert_order_table(self, csv_file_path):\n",
    "        if self.initialized:\n",
    "            print(f\"Tiến hành chèn dữ liệu từ {csv_file_path} vào bảng order...\", end='')\n",
    "            df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "            df = df.select(\n",
    "                df.order_id.cast(\"string\"),\n",
    "                df.customer_id.cast(\"string\"),\n",
    "                df.order_status.cast(\"string\"),\n",
    "                df.order_purchase_timestamp.cast(\"timestamp\"),\n",
    "                df.order_approved_at.cast(\"timestamp\"),\n",
    "                df.order_delivered_carrier_date.cast(\"timestamp\"),\n",
    "                df.order_delivered_customer_date.cast(\"timestamp\"),\n",
    "                df.order_estimated_delivery_date.cast(\"timestamp\")\n",
    "            )\n",
    "            df.write.mode(\"append\").insertInto(f\"{self.catalog}.{self.db_name}.orders_bz\")\n",
    "            print(\"Hoàn thành!\")\n",
    "        else:\n",
    "            raise ReferenceError(\"Không thể xác định được Application database. Không thể tạo bảng trong database}\")\n",
    "\n",
    "    def insert_order_item_table(self, csv_file_path):\n",
    "            if self.initialized:\n",
    "                print(f\"Tiến hành chèn dữ liệu từ {csv_file_path} vào bảng order_item...\", end='')\n",
    "                df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "                df = df.select(\n",
    "                    df.order_id.cast(\"string\"),\n",
    "                    df.order_item_id.cast(\"int\"),\n",
    "                    df.product_id.cast(\"string\"),\n",
    "                    df.seller_id.cast(\"string\"),\n",
    "                    df.shipping_limit_date.cast(\"timestamp\"),\n",
    "                    df.price.cast(\"double\"),\n",
    "                    df.freight_value.cast(\"double\")\n",
    "                )\n",
    "                df.write.mode(\"append\").insertInto(f\"{self.catalog}.{self.db_name}.order_items_bz\")\n",
    "                print(\"Hoàn thành!\")\n",
    "            else:\n",
    "                raise ReferenceError(\"Không thể xác định được Application database. Không thể tạo bảng trong database}\")\n",
    "    \n",
    "    def load_customers(self):\n",
    "            schema = \"\"\"\n",
    "                customer_id string,\n",
    "                customer_unique_id string,\n",
    "                customer_city string,\n",
    "                customer_state string\n",
    "            \"\"\"\n",
    "\n",
    "            df = (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .schema(schema)\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .load(self.landing_zone + \"/customers.csv\"))\n",
    "\n",
    "            full_table_name = f\"{self.catalog}.{self.db_name}.customer_bz\"\n",
    "\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_table_name)\n",
    "\n",
    "            print(\"Batch load to customer completed.\")\n",
    "    \n",
    "    def insert_product_category_name_translation(self, csv_file_path):\n",
    "        if self.initialized:\n",
    "            print(f\"Tiến hành chèn dữ liệu từ {csv_file_path} vào bảng product_category_name_translation...\", end='')\n",
    "            df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "            df = df.select(\n",
    "                df.product_category_name.cast(\"string\"),\n",
    "                df.product_category_name_english.cast(\"string\")\n",
    "            )\n",
    "            df.write.mode(\"append\").insertInto(f\"{self.catalog}.{self.db_name}.category_translation_bz\")\n",
    "            print(\"Hoàn thành!\")\n",
    "        else:\n",
    "            raise ReferenceError(\"Không thể xác định được Application database. Không thể tạo bảng trong database}\")\n",
    "\n",
    "    def load_sellers(self):\n",
    "        schema = \"\"\"\n",
    "            seller_id string,\n",
    "            seller_zip_code_prefix int,\n",
    "            seller_city string,\n",
    "            seller_state string\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/sellers.csv\"))\n",
    "\n",
    "        full_table_name = f\"{self.catalog}.{self.db_name}.sellers_bz\"\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "\n",
    "        print(\"Batch load to seller completed.\")\n",
    "\n",
    "    def load_order_payments(self):\n",
    "        schema = \"\"\"\n",
    "            order_id string,\n",
    "            payment_sequential tinyint,\n",
    "            payment_type string,\n",
    "            payment_installments tinyint,\n",
    "            payment_value float\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/order_payments.csv\"))\n",
    "\n",
    "        full_table_name = f\"{self.catalog}.{self.db_name}.order_payment_bz\"\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "\n",
    "        print(\"Batch load to order_payments completed.\")\n",
    "\n",
    "    def load_order_reviews(self):\n",
    "        schema = \"\"\"\n",
    "            review_id string,\n",
    "            order_id string,\n",
    "            review_score tinyint,\n",
    "            review_comment_title string,\n",
    "            review_comment_message string,\n",
    "            review_creation_date timestamp,\n",
    "            review_answer_timestamp timestamp\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "                .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "                .load(self.landing_zone + \"/order_reviews.csv\"))\n",
    "\n",
    "        full_table_name = f\"{self.catalog}.{self.db_name}.order_reviews_bz\"\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "\n",
    "        print(\"Batch load to reviews completed.\")\n",
    "\n",
    "    def insert_product_data(self, csv_file_path):\n",
    "        if self.initialized:\n",
    "            print(f\"Tiến hành chèn dữ liệu từ {csv_file_path} vào bảng product...\", end='')\n",
    "            df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "            df = df.select(\n",
    "                df.product_id.cast(\"string\"),\n",
    "                df.product_category_name.cast(\"string\"),\n",
    "                df.product_name_length.cast(\"double\").cast(\"int\").alias(\"product_name_length\"),\n",
    "                df.product_description_length.cast(\"double\").cast(\"int\").alias(\"product_description_length\"),\n",
    "                df.product_photos_qty.cast(\"double\").cast(\"int\").alias(\"product_photos_qty\"),\n",
    "                df.product_weight_g.cast(\"double\"),\n",
    "                df.product_length_cm.cast(\"double\"),\n",
    "                df.product_height_cm.cast(\"double\"),\n",
    "                df.product_width_cm.cast(\"double\")\n",
    "            )\n",
    "            df.write.mode(\"append\").insertInto(f\"{self.catalog}.{self.db_name}.products_bz\")\n",
    "            print(\"Hoàn thành!\")\n",
    "        else:\n",
    "            raise ReferenceError(\"Không thể xác định được Application database. Không thể chèn dữ liệu.\")\n",
    "    \n",
    "\n",
    "    def setup(self):\n",
    "        self.create_db()\n",
    "        self.insert_order_table(\"abfss://project-unmanage@gr8sta.dfs.core.windows.net/data_zone/orders.csv\")\n",
    "        self.insert_order_item_table(\"abfss://project-unmanage@gr8sta.dfs.core.windows.net/data_zone/order_items.csv\")\n",
    "        self.insert_product_category_name_translation(\"abfss://project-unmanage@gr8sta.dfs.core.windows.net/data_zone/product_category_name_translation.csv\")\n",
    "        self.insert_product_data(\"abfss://project-unmanage@gr8sta.dfs.core.windows.net/data_zone/products.csv\")\n",
    "        self.load_customers()\n",
    "        self.load_sellers()\n",
    "        self.load_order_payments()\n",
    "        self.load_order_reviews()\n",
    "\n",
    "setup = Bronze(env=\"dev\")\n",
    "setup.setup()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
